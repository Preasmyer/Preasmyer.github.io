---
title: "Project 2 - Part 2: Combined Code"
author: "Team 4 - DS 6030"
date: "2025-04-16"
output: pdf_document:
  toc: true
  toc_depth: 2
  toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of Contents

### 1. Libraries, Loading Data, Cleaning, and Wrangling

  **1.1** - Libraries (102-113)
  
  **1.2** - Cleaning Script (116-187)

### 2. Defining Functions

  **2.1** - Threshold Scanner (198-218)
  
  **2.2** - Test Augment Metric Scanner (221-277)
  
  **2.3** - Custom Metrics Function (280-295)

### 3. EDA Visualizations

  **3.1** - Pixel Box-Plot Comparison (300-333)
  
  **3.2** - Density Plots - Pre|Post Augmentation (336-377)
  
  **3.3** - Augmentation Scanner Results (380-426)
  
  **3.4** - 3D Plot (430-457)
  
  **3.5** - LDA Model Metrics at Thresholds Plot (460-476)

### 4. Model Building - Non-Tunable Models

  **4.1** - QDA, LDA, Log Reg Model Building and fitting (482-515)
  
  **4.2** - PR Visualization between models (518-532)
  
### 5. Model Building - Tunable Models - SVM

  **5.1** - SVM - Defining Models - All Kernels (548-545)
  
  **5.2** - SVM - Hyperparameter Tuning - Polynomial Kernel (548-565)
  
  **5.3** - SVM - Hyperparameter Tuning - Linear Kernel (568-582)
  
  **5.4** - SVM - Hyperparameter Tuning - Radial Basis Function Kernel (585-600)
  
  **5.5** - SVM - Tuning Results & Finalizing Workflows - All Kernels (603-613)
  
  **5.6** - SVM - Training Models and Fitting Cross Validation Folds (616-624)
  
  **5.7** - SVM - Using Threshold Scanner - All Kernels (628-657)
  
  **5.8** - SVM - Making Predictions and Visualizing Results - PR Curve (660-696)
  
### 6. Model Building - Tunable Models - Random Forest

  **6.1** - RF - Defining Workflow and Tuning Model (701-720)
  
  **6.2** - RF - Comparing CV and Test Metrics (724-748)
  
  **6.3** - RF - Threshold Scan Results (751-754)

### 7. Model Building - Tunable Models - Logistic Regression

  **7.1** - LogReg - Defining Workflow and Tuning Model (760-777)
  
  **7.2** - LogReg - Comparing CV and Test Metrics (782-805)
  
  **7.3** - LogReg - Threshold Scan Results (810-813)

### 8. Model Building - Tunable Models - K-Nearest Neighbors

  **8.1** - KNN - Model Building (818-838)
  
  **8.2** - KNN - Comparing CV and Test Metrics (842-865)
  
  **8.3** - KNN - Threshold Scan Results (868-871)
  
### 9. Tunable Model Comparison

  **9.1** - CV vs Test Performance - All Tunable Models (876-917)

  **9.2** - Comparing Tunable Model PR Curves with Threshold Points (920-938)
  
  **9.3** - Function for finding x given lambda (941-955)

# 1. Libraries, Loading Data, Cleaning, and Wrangling

**1.1: Libraries**
```{r}
#| warning: false
#| message: false
library(tidymodels)
library(kernlab)
library(probably)
library(MASS)
library(pROC)
library(plotly)
library(gridExtra)
library(doParallel)
```

**1.2: Cleaning Script**
```{r}
# Preparing column names for cleaning
colnames <- c('ID', 'X', 'Y', 'Map X','Map Y', 'Lat', 'Lon', 'B1', 'B2', 'B3')

# Reading in data (Change File-path to Yours) (Excluding BT01)
train <- read.csv("~/Desktop/DS 6030/Project 1/HaitiPixels.csv")
BT02 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/BT02.txt", 
                   header = F, skip=8, col.names = colnames)
BT03 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/BT03.txt", 
                   header = F, skip = 8, col.names = colnames)
BT04 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/BT04.txt", 
                   header = F, skip=8, col.names = colnames)
NBT01 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/NBT01.txt", 
                    header = F, skip=8, col.names=colnames)
NBT02 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/NBT02.txt", 
                    header = F, skip=8, col.names=colnames)
NBT03 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/NBT03.txt", 
                    header = F, skip=8, col.names=colnames)
NBT04 <- read.table("~/Desktop/DS 6030/Project 1/HoldOutData/NBT04.txt", 
                    header = F, skip=8, col.names=colnames)

# Defining the Cleaning Function
library(tidymodels)
BTclean <- function(df){
  df <- df %>%
    dplyr::select(B1,B2,B3) %>% 
    rename('Red' = 'B1',
           'Green'='B2',
           'Blue' = 'B3') %>% 
    mutate('Class' = 'Blue Tarp',.before ='Red')
    return(df)
}
NBTclean <- function(df){
  df <- df %>%
    dplyr::select(B1,B2,B3) %>% 
    rename('Red' = 'B1',
           'Green'='B2',
           'Blue' = 'B3') %>% 
    mutate('Class' = 'Other',.before ='Red')
    return(df)
}

# Applying Cleaning Function
BT02 <- BTclean(BT02)
BT03 <- BTclean(BT03)
BT04 <- BTclean(BT04)
NBT01 <- NBTclean(NBT01)
NBT02 <- NBTclean(NBT02)
NBT03 <- NBTclean(NBT03)
NBT04 <- NBTclean(NBT04)

# Making initial test and train sets from data & making factors
test <- rbind(NBT01, BT02, NBT02, BT03, NBT03, BT04, NBT04)
train$Class <- ifelse(train$Class == 'Blue Tarp', 'Blue Tarp', 'Other')
test$Class <- as.factor(test$Class)
train$Class <- as.factor(train$Class)

# Augmenting Data to Training Set
set.seed(6034)
add <- test[sample(nrow(test), 1000000, replace =F ),]
add <- add %>% filter(Class == 'Blue Tarp')
set.seed(6034)
test <- test[-sample(nrow(test), 1000000, replace =F ),]
train <- rbind(train, add)

#Cleaning the Environment of the train/test set building blocks
rm(BT02, BT03, BT04, NBT01, NBT02, NBT03, NBT04, add, colnames, BTclean, NBTclean)

# Creating CV Folds
set.seed(6034)
folds <- vfold_cv(train, strata = 'Class')
```

**(Optional) Register Cluster for faster computation**
```{r}
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

# 2. Important Functions

**2.1: Threshold Scanning Function**
```{r}
threshold_scan <- function(augmented_model, low, high, by){
  df1 <- data.frame(i=numeric(),accuracy=numeric(), precision = numeric(), TPR = numeric(), FPR = numeric())
  for (i in seq(low, high, by)){
    augmented_model$.pred_class<- ifelse(augmented_model$'.pred_Blue Tarp' >= i,'Blue Tarp', 'Other')
    TP <- sum(augmented_model$.pred_class == 'Blue Tarp' & augmented_model$Class == 'Blue Tarp')
    TN <- sum(augmented_model$.pred_class == 'Other' & augmented_model$Class == 'Other')
    FP <- sum(augmented_model$.pred_class == 'Blue Tarp' & augmented_model$Class == 'Other')
    FN <- sum(augmented_model$.pred_class == 'Other' & augmented_model$Class == 'Blue Tarp')
    accuracy <- (TP+TN) / (TP+TN+FP+FN)
    precision <- TP/(TP+FP)
    TPR <- TP/(TP+FN)
    FPR <- FP/(FP+TN)
    value <- 1 + (TP/FP)
    ## Maybe add in a part that shows value of blue tarp preds divided by FP's
    df <- data.frame(i, accuracy, precision, TPR, FPR, value)
    df1 <- rbind(df1, df)
  }
  return(df1)
}
```

**2.2: Test Augment Metric Scanner**
```{r}
test_augment_metric_scan <- function(train, test, seq_low, seq_high, 
                                     seq_interval, seed, workflow, base_metric){
  
  # Creating df to append to
  results_df <- data.frame(n_augmented = numeric(), z = numeric(), 
                           TP= numeric(), FP= numeric(), TN= numeric(), 
                           FN=numeric(),precision= numeric(), 
                           accuracy= numeric(), recall= numeric(), 
                           f_meas= numeric())
  
  for (i in seq(seq_low, seq_high, seq_interval)){
    # Getting Count of Blue Tarp observations from base test set
    x <- sum(test$Class == 'Blue Tarp')
    # Adding sampled data to object (will add to training set later)
    set.seed(seed)
    add <- test[sample(nrow(test), i, replace =F ),]
    # Extracting the same data from test set to avoid double representation of observations
    set.seed(seed)
    test1 <- test[-sample(nrow(test), i, replace =F ),]
    # Filtering out Blue Tarp Observations and discarding non-blue tarp obs
    add <- add %>% 
      filter(Class == 'Blue Tarp')
    # Augmenting training set with extracted test data
    train1 <- rbind(train, add)
    # Finding new number of Blue Tarp observations to calculate %improvement from base
    y <- sum(test1$Class == 'Blue Tarp')
    # Fitting new training set to the workflow and applying to the new test set
    fit <- workflow %>% 
      fit(train1)
    aug <- augment(fit, new_data = test1)
    
    # Gathering Conf_Mat values at base threshold (50%) and 
    # comparing to base test/train FNR before augmenting (170 FNR)
    TP <- sum(aug$.pred_class == 'Blue Tarp' & aug$Class == 'Blue Tarp')
    FP <- sum(aug$.pred_class == 'Blue Tarp' & aug$Class == 'Other')
    TN <- sum(aug$.pred_class == 'Other' & aug$Class == 'Other')
    FN <- sum(aug$.pred_class == 'Other' & aug$Class == 'Blue Tarp')
    precision <- TP/(FP+TP)
    accuracy <- (TP + TN)/(TP + TN + FN + FP)
    recall <- TP/ (TP+FN)
    f_meas <- (2*(precision*recall))/(precision+recall)
    # Gathering Expected FPR based on %- in test obs, and appending to df
    # (expected FPR if observations are taken out and model does not change.)
    expected_FN <- (1-((x-y)/x))*base_metric$FN
    FN_imprv <- expected_FN-FN
    train_size <- sum(train1$Class == 'Blue Tarp')
    results <- data.frame('n'=i,'FN_Imprvmt' = FN_imprv,'new_train_size' =
                            train_size,TP,FP,TN,FN,precision,accuracy,recall,f_meas)
    results_df <- rbind(results_df, results)
    # Resetting test and train for next iteration
    test <- test
    train <- train
  }
  return(results_df)
}
```

**2.3: Custom Metrics Function**
```{r}
custom_metrics <- function(augmented_model, threshold=.5){
  augmented_model$.pred_class<- ifelse(augmented_model$'.pred_Blue Tarp' >= threshold,'Blue Tarp', 'Other')
  TP <- sum(augmented_model$.pred_class == 'Blue Tarp' & augmented_model$Class == 'Blue Tarp')
  TN <- sum(augmented_model$.pred_class == 'Other' & augmented_model$Class == 'Other')
  FP <- sum(augmented_model$.pred_class == 'Blue Tarp' & augmented_model$Class == 'Other')
  FN <- sum(augmented_model$.pred_class == 'Other' & augmented_model$Class == 'Blue Tarp')
  accuracy <- (TP+TN) / (TP+TN+FP+FN)
  precision <- TP/(TP+FP)
  TPR <- TP/(TP+FN)
  FPR <- FP/(FP+TN)
  val <- 1+(TP/FP)
  df <- data.frame(threshold, accuracy, precision, TPR, FPR, val, TP, FP, TN, FN)
  return(df)
}
```

# 3. EDA Vizualizations

**3.1: Pixel Box-Plot Comparison**
```{r}
# Red Box
red_box <- train %>%
  ggplot(aes(x = Class, y = Red, fill = Class)) +
  geom_boxplot(color='#b185b4') +
  labs(title = "Red Intensity", y = "Red Values",
       x = "") +
  scale_fill_manual(values = c("Blue Tarp" = "lightblue", "Other" = "lightgrey")) +
  theme_minimal() +
  theme(legend.position = "right", axis.text.x = element_blank())

# Green Box
green_box <- train %>%
  ggplot(aes(x = Class, y = Green, fill = Class)) +
  geom_boxplot(color='#b185b4') +
  labs(title = "Green Intensity", y = "Green Values",
       x = "") +
  scale_fill_manual(values = c("Blue Tarp" = "lightblue", "Other" = "lightgrey")) +
  theme_minimal() +
  theme(legend.position = "right", axis.text.x = element_blank())

# Blue Box
blue_box <- train %>%
  ggplot(aes(x = Class, y = Blue, fill = Class)) +
  geom_boxplot(color='#b185b4') +
  labs(title = "Blue Intensity", y = "Blue Values",
       x = "") +
  scale_fill_manual(values = c("Blue Tarp" = "lightblue", "Other" = "lightgrey")) +
  theme_minimal() +
  theme(legend.position = "right", axis.text.x = element_blank())

# Combine Plots
grid.arrange(red_box, green_box, blue_box, ncol = 3)
```

**3.2: Density Plots - Pre|Post Augmentation**
```{r}
# Create Subset with only Blue Tarp pixels for comparison
train_blue_tarp <- train %>% filter(Class == "Blue Tarp")
test_blue_tarp <- test %>% filter(Class == "Blue Tarp")

# Derive Averages for comparison on the Density Plots
train_averages <- train_blue_tarp %>%
  summarise(Red_Avg = mean(Red), Green_Avg = mean(Green), Blue_Avg = mean(Blue))
test_averages <- test_blue_tarp %>%
  summarise(Red_Avg = mean(Red), Green_Avg = mean(Green), Blue_Avg = mean(Blue))

# Create Density Plots
red_density <- ggplot() +
  geom_density(data = train_blue_tarp, aes(x = Red, color = "Training Set"), size = 1) +
  geom_density(data = test_blue_tarp, aes(x = Red, color = "Test Set"), size = 1) +
  geom_vline(aes(xintercept = train_averages$Red_Avg, color = "Training Set"), linetype = "dashed") +
  geom_vline(aes(xintercept = test_averages$Red_Avg, color = "Test Set"), linetype = "dashed") +
  labs(title = "Red Pixel Intensity Density (Blue Tarp)", x = "Red Values", y = "Density") +
  scale_color_manual(values = c("Training Set" = "blue", "Test Set" = "lightblue3")) +
  theme_minimal()

green_density <- ggplot() +
  geom_density(data = train_blue_tarp, aes(x = Green, color = "Training Set"), size = 1) +
  geom_density(data = test_blue_tarp, aes(x = Green, color = "Test Set"), size = 1) +
  geom_vline(aes(xintercept = train_averages$Green_Avg, color = "Training Set"), linetype = "dashed") +
  geom_vline(aes(xintercept = test_averages$Green_Avg, color = "Test Set"), linetype = "dashed") +
  labs(title = "Green Pixel Intensity Density (Blue Tarp)", x = "Green Values", y = "Density") +
  scale_color_manual(values = c("Training Set" = "blue", "Test Set" = "lightblue3")) +
  theme_minimal()

blue_density <- ggplot() +
  geom_density(data = train_blue_tarp, aes(x = Blue, color = "Training Set"), size = 1) +
  geom_density(data = test_blue_tarp, aes(x = Blue, color = "Test Set"), size = 1) +
  geom_vline(aes(xintercept = train_averages$Blue_Avg, color = "Training Set"), linetype = "dashed") +
  geom_vline(aes(xintercept = test_averages$Blue_Avg, color = "Test Set"), linetype = "dashed") +
  labs(title = "Blue Pixel Intensity Density (Blue Tarp)", x = "Blue Values", y = "Density") +
  scale_color_manual(values = c("Training Set" = "blue", "Test Set" = "lightblue3")) +
  theme_minimal()

# Concatinate together
grid.arrange(red_density, green_density, blue_density, ncol = 1)
```

**3.3: Augmentation Scan Prep & Visualization**
```{r}
# Creating WF's for visualization
formula <- Class ~.
lda_wf <- workflow() %>% 
  add_model(discrim_linear(mode='classification', engine ='MASS')) %>% 
  add_formula(formula)
qda_wf <- workflow() %>% 
  add_model(discrim_quad(mode='classification', engine ='MASS')) %>% 
  add_formula(formula)
log_wf <- workflow() %>% 
  add_model(logistic_reg(mode='classification', engine = 'glm' )) %>% 
  add_formula(formula)

# Creating Base Metrics
lda_base <- base_metrics(train, test, lda_wf)
qda_base <- base_metrics(train, test, qda_wf)
log_base <- base_metrics(train, test, log_wf)

# Using Augmentation Scanner to derive metrics across augmentation amounts
lda_results <- test_augment_metric_scan(train, test, 500000, 1200000, 100000, 6034, lda_wf, lda_base)
lda_results
qda_results <- test_augment_metric_scan(train, test, 500000, 1200000, 100000, 6034, qda_wf, qda_base)
qda_results
log_results <- test_augment_metric_scan(train, test, 500000, 1200000, 50000, 6034, nt_log_wf, log_base)
log_results

# Prepping to map
lda_plot_prep <- lda_results %>% 
  mutate(model = 'LDA')
qda_plot_prep <- qda_results %>% 
  mutate(model = 'QDA')
log_plot_prep <- log_results %>% 
  mutate(model = 'Log')

# Combining values
combined_plot <- rbind(lda_plot_prep, qda_plot_prep, log_plot_prep)

# Plotting
ggplot(combined_plot, aes(x = n, y = recall, color = model)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  labs(title = "Recall vs. # Augmented Observations, All Models",
       x = "Number of Observations Augmented to Training set from Test set",
       y = "Recall") +
  theme_minimal()+
  theme(plot.title = element_text(hjust=0.5))
```


**3.4: 3D Plot**
```{r}
library(plotly)

# Create a 3D scatter plot
plot_3d <- plot_ly(
  data = train,
  x = ~Red,
  y = ~Green,
  z = ~Blue,
  color = ~Class,  # Color points by Class
  colors = c("Blue Tarp" = "blue", "Other" = "red"),
  alpha=0.9,
  type = "scatter3d",
  mode = "markers",
  marker= list(size=0.5)
) %>%
  layout(
    title = "3D Scatter Plot of Classes - RBG Intensities",
    scene = list(
      xaxis = list(title = "Red"),
      yaxis = list(title = "Green"),
      zaxis = list(title = "Blue")
    )
  )

# Display the 3D plot
plot_3d
```

**3.5: LDA Model Metrics at Threshold Plot**
```{r}
lda_fit <- lda_wf %>% 
  fit(data=train)
lda_aug <- augment(lda_fit, new_data=test)
threshold_scan_df <- threshold_scan(lda_aug, 0.0,1,0.001)
max_acc <- threshold_scan_df[which.max(threshold_scan_df$accuracy),]$i
max_tpr <- threshold_scan_df[which.max(threshold_scan_df$TPR),]$i
df_long <- pivot_longer(threshold_scan_df, cols = c('accuracy', 'precision', 'TPR','FPR'), names_to = "variable", values_to = "value")
 ggplot(df_long, aes(x=i, y=value, color = variable))+
  geom_point(size=1, shape =3)+
  geom_vline(xintercept = 0.979, linetype = 2, color = 'darkred')+
  theme_minimal()+
  labs(title = 'LDA Model Metrics at Various Thresholds (0-1)',
       x='Threshold',
       y= 'Value')+
  theme(plot.title = element_text(hjust= 0.5))
```


# 4. Model Building - Non-Tunable Models

**4.1: QDA, LDA, Log Reg Model Building and fitting**
```{r}
# Workflows
lda_wf <- workflow() %>% 
  add_model(discrim_linear(mode='classification', engine ='MASS')) %>% 
  add_formula(formula)
qda_wf <- workflow() %>% 
  add_model(discrim_quad(mode='classification', engine ='MASS')) %>% 
  add_formula(formula)
nt_log_wf <- workflow() %>% 
  add_model(logistic_reg(mode='classification', engine = 'glm' )) %>% 
  add_formula(formula)

# Fitting data
lda_fit <- lda_wf %>% fit(train)
qda_fit <- qda_wf %>% fit(train)
nt_log_fit <- nt_log_wf %>% fit(train)

# Fitting CV folds
cv_control <- control_resamples(save_pred=TRUE)
lda_cv <- lda_wf %>% fit_resamples(folds, control = cv_control) %>% collect_predictions()
qda_cv <- qda_wf %>% fit_resamples(folds, control = cv_control) %>% collect_predictions()
log_cv <- log_wf %>% fit_resamples(folds, control = cv_control) %>% collect_predictions()

# Making Predictions for Test set and CV folds
lda_pr <- augment(lda_fit, new_data=test) %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
qda_pr <- augment(qda_fit, new_data=test) %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
log_pr <- augment(log_fit, new_data=test) %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
lda_cv_pr <- lda_cv %>% pr_curve(Class, '.pred_Blue Tarp', event_level="first")
qda_cv_pr <- qda_cv %>% pr_curve(Class, '.pred_Blue Tarp', event_level="first")
log_cv_pr <- log_cv %>% pr_curve(Class, '.pred_Blue Tarp', event_level="first")
```

**4.2: PR Visualization between models**
```{r}
ggplot() +
  geom_path(data = lda_pr, aes(x = recall, y = precision, color = "LDA", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = lda_cv_pr, aes(x = recall, y = precision, color = "LDA", linetype = "CV"), linewidth = 0.7) +
  geom_path(data = qda_pr, aes(x = recall, y = precision, color = "QDA", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = qda_cv_pr, aes(x = recall, y = precision, color = "QDA", linetype = "CV"), linewidth = 0.7) +
  geom_path(data = log_pr, aes(x = recall, y = precision, color = "Logistic Reg", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = log_cv_pr, aes(x = recall, y = precision, color = "Logistic Reg", linetype = "CV"), linewidth = 0.7) +
  scale_color_manual(values = c("LDA" = "blue","QDA" = "green2","Logistic Reg" = "red2")) +
  scale_linetype_manual(values = c("Test" = "dotdash", "CV" = "solid")) +
  labs(title = 'Non-Tunable Model Comparison - CV vs Test Set', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


# 5. Model Building - Tunable Models

**5.1: SVM - Defining Models - All Kernels**
```{r}
## Formula
formula <- Class ~.
## Base Models
svm_polyn <- svm_poly(mode='classification', engine = 'kernlab', cost=tune(), margin=tune(), degree=tune())
svm_linear <- svm_linear(mode='classification', engine='kernlab', cost=tune(), margin=tune())
svm_radial <- svm_rbf(mode='classification', engine='kernlab', cost=tune(), margin=tune(), rbf_sigma=tune())
```

**5.2: SVM - Hyperparameter Tuning - Polynomial Kernel**
```{r}
start <- Sys.time()
set.seed(6034)
cv_metrics <- metric_set(roc_auc, accuracy)
poly_tune_wf <- workflow() %>% 
  add_model(svm_polyn) %>% 
  add_formula(formula)
poly_params <- extract_parameter_set_dials(poly_tune_wf) %>% 
  update(degree = degree_int(range=c(2,5)))
poly_tune <- tune_bayes(poly_tune_wf,
                         resamples=folds,
                         metrics=cv_metrics,
                         param_info=poly_params,
                         iter=25
                         )
autoplot(poly_tune)
end <- Sys.time()
```

**5.3: SVM - Hyperparameter Tuning - Linear Kernel**
```{r}
set.seed(6034)
cv_metrics <- metric_set(roc_auc, accuracy)
linear_tune_wf <- workflow() %>% 
  add_model(svm_linear) %>% 
  add_formula(formula)
linear_params <- extract_parameter_set_dials(linear_tune_wf)
linear_tune <- tune_bayes(linear_tune_wf,
                         resamples=folds,
                         metrics=cv_metrics,
                         param_info=linear_params,
                         iter=25
                         )
autoplot(linear_tune)
```

**5.4: SVM - Hyperparameter Tuning - Radial Basis Function Kernel**
```{r}
set.seed(6034)
cv_metrics <- metric_set(roc_auc, accuracy)
rbf_tune_wf <- workflow() %>% 
  add_model(svm_radial) %>% 
  add_formula(formula)
rbf_params <- extract_parameter_set_dials(rbf_tune_wf) %>% 
  update(rbf_sigma = rbf_sigma(range=c(-4, 0), trans=log10_trans()))
rbf_tune <- tune_bayes(rbf_tune_wf,
                         resamples=folds,
                         metrics=cv_metrics,
                         param_info=rbf_params,
                         iter=25
                         )
autoplot(rbf_tune)
```

**5.5: SVM - Tuning Results & Finalizing Workflows - All Kernels**
```{r}
select_best(rbf_tune, metric='roc_auc')
select_best(linear_tune, metric='roc_auc')
select_best(poly_tune, metric='roc_auc')
svm_lin_wf <- linear_tune_wf %>% 
  finalize_workflow(select_best(linear_tune, metric='roc_auc'))
svm_poly_wf <- poly_tune_wf %>% 
  finalize_workflow(select_best(poly_tune, metric = 'roc_auc'))
svm_rbf_wf <- rbf_tune_wf %>% 
  finalize_workflow(select_best(rbf_tune, metric='roc_auc'))
```

**5.6: SVM - Training Models and Fitting Cross Validation Folds**
```{r}
cv_control <- control_resamples(save_pred=TRUE)
lin_cv <- fit_resamples(svm_lin_wf, folds, metrics=cv_metrics, control=cv_control)
poly_cv <- fit_resamples(svm_poly_wf, folds, metrics=cv_metrics, control=cv_control)
rbf_cv <- fit_resamples(svm_rbf_wf, folds, metrics=cv_metrics, control=cv_control)
lin_fit <- svm_lin_wf %>% fit(data=train)
poly_fit <- svm_poly_wf %>% fit(data=train)
rbf_fit <- svm_rbf_wf %>% fit(data=train)
```


**5.7: SVM - Using Threshold Scanner - All Kernerls**
```{r}
#| warning: false
#| message: false
# Augmenting Data with Test Set for Predictions
poly_base_preds <- augment(poly_fit, new_data=test)
lin_base_preds <- augment(lin_fit, new_data=test)
rbf_base_preds <- augment(rbf_fit, new_data=test)
poly_scan_results <- threshold_scan(poly_base_preds, 0, 1,0.001)
lin_scan_results <- threshold_scan(lin_base_preds, .0, 1, 0.0001)
rbf_scan_results <- threshold_scan(rbf_base_preds, 0, 1, 0.001)

# Choosing threshold based on CV
lin_scan_results <- threshold_scan(lin_cv %>% collect_predictions(), 0, 1, 0.001)
# Collecting Pred Threshold
custom_metrics(lin_base_preds, threshold = .994)

poly_scan_results$model <- "Polynomial SVM"
lin_scan_results$model <- "Linear SVM"
rbf_scan_results$model <- "RBF SVM"
scan_results <- bind_rows(poly_scan_results, lin_scan_results, rbf_scan_results)

ggplot(scan_results, aes(x = i, y = value, color = model)) +
  geom_point(shape = 5, alpha = 0.8, size = 0.8) +
  labs(title = "Threshold Scan Results",
       x = "Threshold",
       y = "1 + TP / FP",
       color = "Model Type") +
  theme_minimal()+
  theme(plot.title = element_text(hjust=0.5))
```

**5.8: SVM - Making Predictions and Visualizing Results - PR Curve**
```{r}
svm_preds <- collect_predictions(lin_cv)
svm_preds_poly <- collect_predictions(poly_cv)
svm_preds_rbf <- collect_predictions(rbf_cv)

# Cross Validation Results
svm_pr_cv_poly<- svm_preds_poly %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
svm_pr_cv_rbf<- svm_preds_rbf %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
svm_pr_cv <- svm_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Test set results
svm_pr_test <- lin_base_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
svm_pr_test_rbf <- rbf_base_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
svm_pr_test_poly <- poly_base_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Vizualizing CV vs Test performance on PR Curves
ggplot() +
  geom_path(data = svm_pr_test, aes(x = recall, y = precision, color = "Linear", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = svm_pr_cv, aes(x = recall, y = precision, color = "Linear", linetype = "CV"), linewidth = 0.7) +
  geom_path(data = svm_pr_test_poly, aes(x = recall, y = precision, color = "Poly", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = svm_pr_cv_poly, aes(x = recall, y = precision, color = "Poly", linetype = "CV"), linewidth = 0.7) +
  geom_path(data = svm_pr_test_rbf, aes(x = recall, y = precision, color = "RBF", linetype = "Test"), linewidth = 0.7) +
  geom_path(data = svm_pr_cv_rbf, aes(x = recall, y = precision, color = "RBF", linetype = "CV"), linewidth = 0.7) +
  scale_color_manual(values = c("Linear" = "red2","Poly" = "green2","RBF" = "blue")) +
  scale_linetype_manual(values = c("Test" = "dotdash", "CV" = "solid")) +
  labs(title = 'SVM Kernels - Precision-Recall Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.75, 1), ylim = c(0.75, 1))
```

# 6. Model Building - Tunable Models - Random Forest

**6.1: RF - Defining Workflow and Tuning Model**
```{r}
set.seed(6034)
rf_wf <- workflow() %>%
    add_recipe(recipe(formula, data=train)) %>%
    add_model(rand_forest(mode="classification", mtry=tune(), min_n=tune(), trees = tune()) %>%
            set_engine("ranger", importance="impurity"))
rf_parameters <- extract_parameter_set_dials(rf_wf)%>%
    update(mtry = mtry(c(1, 3)))
rf_tune <- tune_bayes(rf_wf,
    resamples=folds,
    metrics=cv_metrics,
    param_info=rf_parameters, iter=25)

autoplot(rf_tune)

select_best(rf_tune, metric='roc_auc')

final_rf_wf <- rf_wf %>% 
  finalize_workflow(select_best(rf_tune, metric='roc_auc'))
```

**6.2: RF - Comparing CV and Test Metrics**

```{r}
cv_control <- control_resamples(save_pred=TRUE)
rf_fit <- final_rf_wf %>% fit(train)
rf_cv <- fit_resamples(final_rf_wf, folds, metrics=cv_metrics, control=cv_control) %>% 
  collect_predictions()
rf_preds <- augment(rf_fit, new_data=test)

# Cross Validation Results in PR format
rf_cv_pr<- rf_cv %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
# Test set results in PR format
rf_test_pr <- rf_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Plotting Results
ggplot() +
  geom_path(data = rf_test_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = rf_cv_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'solid') +
  scale_color_manual(values = c("RF" = "lightblue")) +
  labs(title = 'Random Forest PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.75, 1), ylim = c(0.75, 1))
```

**6.3: RF - Threshold Scan results**
```{r}
rf_thres <- threshold_scan(rf_cv, 0, 1,0.001)
custom_metrics(rf_preds, threshold=0.998)
```

# 7. Model Building - Tunable Models - Logistic Regression

**7.1: LogReg - Defining Workflow and Tuning Model**
  
```{r}
logreg_wf <- workflow() %>%
    add_recipe(recipe(formula, data=train)) %>%
    add_model(logistic_reg(engine="glmnet", mode ="classification", penalty=tune()))
logreg_parameters <- extract_parameter_set_dials(logreg_wf)%>%
    update(penalty = penalty(c(-5,5)))
logreg_tune <- tune_grid(logreg_wf,
    resamples=folds,
    metrics=cv_metrics,
    grid = grid_regular(logreg_parameters, levels=300))

autoplot(logreg_tune)

select_best(logreg_tune, metric='roc_auc')

final_logreg_wf <- logreg_wf %>% 
  finalize_workflow(select_best(logreg_tune, metric='roc_auc'))
```
  
  
  **7.2: LogReg - Comparing CV and Test Metrics**
  
```{r}
cv_control <- control_resamples(save_pred=TRUE)
logreg_fit <- final_logreg_wf %>% fit(train)
logreg_cv <- fit_resamples(final_logreg_wf, folds, metrics=cv_metrics, control=cv_control) %>% 
  collect_predictions()
logreg_preds <- augment(logreg_fit, new_data=test)

# Cross Validation Results in PR format
logreg_cv_pr<- logreg_cv %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
logreg_test_pr <- logreg_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Plotting Results
ggplot() +
  geom_path(data = logreg_test_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = logreg_cv_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'solid') +
  scale_color_manual(values = c("RF" = "lightblue")) +
  labs(title = 'Logistic Regression PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.75, 1), ylim = c(0.75, 1))
```
  
  
  **7.3: LogReg - Threshold Scan Results**

```{r}
logreg_thres <- threshold_scan(logreg_cv, 0.99, 1,0.001)
custom_metrics(logreg_preds, threshold=0.9983)
```

# 8. Model Building - Tunable Models - K-Nearest Neighbors

**8.1: KNN - Defining Workflow and Tuning Model**
```{r}
knn_tune_model <- nearest_neighbor(engine="kknn", mode="classification", neighbors = tune())

knn_wf <-workflow() %>% 
  add_formula(formula) %>% 
  add_model(knn_tune_model)

knn_grid <- grid_regular(neighbors(range = c(2, 45)), levels = 43)

knn_tune_results <- tune_grid(
  knn_wf,
  resamples = folds,
  grid = knn_grid,
  metrics = cv_metrics
)
autoplot(knn_tune_results)
select_best(knn_tune_results)

final_knn_wf <- knn_wf %>% 
  finalize_workflow(select_best(knn_tune_results, metric='roc_auc'))
```

**8.2: KNN - Comparing CV and Test Metrics**

```{r}
knn_fit <- final_knn_wf %>% fit(train)
knn_cv <- fit_resamples(final_knn_wf, folds, metrics=cv_metrics, control=cv_control) %>% 
  collect_predictions()
knn_preds <- augment(knn_fit, new_data=test)

# Cross Validation Results in PR format
knn_cv_pr<- knn_cv %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
# Test set results in PR format
knn_test_pr <- knn_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Plotting Results
ggplot() +
  geom_path(data = knn_test_pr, aes(x = recall, y = precision, color = "KNN"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = knn_cv_pr, aes(x = recall, y = precision, color = "KNN"), linewidth = 0.7, linetype = 'solid') +
  scale_color_manual(values = c("KNN" = "lightblue")) +
  labs(title = 'KNN PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.6, 1), ylim = c(0.6, 1))
```

**8.3: KNN - Threshold Scan results**
```{r}
knn_thres <- threshold_scan(knn_cv, 0, 1,0.001)
custom_metrics(knn_preds, threshold=0.997)
```

# 9. Model Comparison

**9.1: CV vs Test Performance - All Tunable Models**
```{r}
svm_preds <- collect_predictions(lin_cv)
lin_base_preds <- augment(lin_fit, new_data=test)
svm_pr_cv <- svm_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")
svm_pr_test <- lin_base_preds %>%
    pr_curve(Class, '.pred_Blue Tarp', event_level="first")

# Stacked Test PR Curve 

a <- ggplot() +
  geom_path(data = svm_pr_test, aes(x = recall, y = precision, color = "SVM"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = rf_test_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = knn_test_pr, aes(x = recall, y = precision, color = "KNN"), linewidth = 0.7, linetype = 'dotdash') +
  geom_path(data = logreg_test_pr, aes(x = recall, y = precision, color = "Logistic Regression"), linewidth = 0.7, linetype = 'dotdash') +
  scale_color_manual(values = c("SVM" = "blue", 'Logistic Regression' = 'purple2', 'RF' = 'forestgreen', 'KNN' = 'orange')) +
  labs(title = 'Tuned Models Test Set Comparison - PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.7, 1), ylim = c(0.7, 1))



# Stacked CV PR Curve
b <- ggplot() +
  geom_path(data = svm_pr_cv, aes(x = recall, y = precision, color = "SVM"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = rf_cv_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = logreg_cv_pr, aes(x = recall, y = precision, color = "Logistic Regression"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = knn_cv_pr, aes(x = recall, y = precision, color = "KNN"), linewidth = 0.7, linetype = 'solid') +
  scale_color_manual(values = c("SVM" = "blue", 'Logistic Regression' = 'purple2', 'RF' = 'forestgreen', 'KNN' = 'orange')) +
  labs(title = 'Tuned Models CV Comparison - PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.9, 1), ylim = c(0.9, 1))

# Saving Images (Feel free to turn into a comment line)

ggsave('/Users/cameronpreasmyer/Desktop/DS\ 6030/Project\ 2/Plot\ Images\ EDA/StackedTEST.png',plot=a)
ggsave('/Users/cameronpreasmyer/Desktop/DS\ 6030/Project\ 2/Plot\ Images\ EDA/StackedCV.png',plot=b)
```

**9.2: Comparing Tunable Model Test Set PR Curves with Threshold Points**
```{r}
c <- ggplot() +
  geom_path(data = svm_pr_test, aes(x = recall, y = precision, color = "SVM"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = logreg_test_pr, aes(x = recall, y = precision, color = "Logistic Regression"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = knn_test_pr, aes(x = recall, y = precision, color = "KNN"), linewidth = 0.7, linetype = 'solid') +
  geom_path(data = rf_test_pr, aes(x = recall, y = precision, color = "RF"), linewidth = 0.7, linetype = 'solid') +
  annotate("point", x = 0.9445, y = 0.8971, color = "blue", size = 6, shape = 10)+
  annotate("point", x = 0.8896, y = 0.9512, color = "forestgreen", size = 6, shape = 10)+
  annotate("point", x = 0.9637, y = 0.7707, color = "orange", size = 6, shape = 10)+
  annotate("point", x = 0.9314, y = 0.9175, color = "purple2", size = 6, shape = 10)+
  scale_color_manual(values = c("SVM" = "blue", 'Logistic Regression' = 'purple2', 'RF' = 'forestgreen', 'KNN' = 'orange')) +
  labs(title = 'Tuned Models CV Comparison - PR Curve', x = 'Recall', y = 'Precision', 
       color = 'Model Type', linetype = 'Dataset') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))+
  coord_cartesian(xlim = c(0.75, 1), ylim = c(0.75, 1))
ggsave('/Users/cameronpreasmyer/Desktop/DS\ 6030/Project\ 2/Plot\ Images\ EDA/tunablePRtestset.png', plot=c)

```

**9.3: Function for finding x given lambda**
```{r}
compute_x <- function(TP, FP, lambda) {
  return(-TP / (FP * (1 - lambda)))
}
TP <- 6551
FP <- 399 
compute_x(TP,FP,5)
lambda_vals <- seq(0.01, 0.99 + TP/FP, by = 0.01)
x_vals <- sapply(lambda_vals, function(lambda) compute_x(TP, FP, lambda))
df <- data.frame(lambda = lambda_vals, x = x_vals)
ggplot(df, aes(x=lambda, y=x))+
  geom_point()+
  ylim(0,30)+
  xlim(0,30)
```


**Unregister Cluster**
```{r}
stopCluster(cl)
registerDoSEQ()
```




